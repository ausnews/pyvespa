{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fine-tune the BERT models for the cord19 application we need to generate a set of query-document features as well as labels that indicate which documents are relevant for the specific queries. For this exercise we will use the `query` string to represent the query and the `title` string to represent the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `labelled_data.json` contains information about the `query` string and the file `training_all_judgement_data.csv` contain information about labels and `title` string. Those files were created and covered elsewhere but you can download them [here](https://drive.google.com/file/d/1R2hZTF6QBKPMaiuS4Du6aXQlBVnfZOAA/view?usp=sharing) and [here](https://drive.google.com/file/d/18jNRM7G7agbO1Mg9t0l1pvqsz-qwEXpz/view?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pandas import read_csv\n",
    "\n",
    "labelled_data = json.load(open(\"labelled_data_all.json\", \"r\"))\n",
    "training_data = read_csv(\"training_all_jugdments_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`training_data` has almost everything we need, except the `query` string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>query_id</th>\n",
       "      <th>label</th>\n",
       "      <th>title-full</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>005b2j4b</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Monophyletic Relationship between Severe Acute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00fmeepz</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Comprehensive overview of COVID-19 based on cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>010vptx3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>The SARS, MERS and novel coronavirus (COVID-19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0194oljo</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Evidence for zoonotic origins of Middle East r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>021q9884</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Deadly virus effortlessly hops species</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  document_id  query_id  label  \\\n",
       "0    005b2j4b         1      2   \n",
       "1    00fmeepz         1      1   \n",
       "2    010vptx3         1      2   \n",
       "3    0194oljo         1      1   \n",
       "4    021q9884         1      1   \n",
       "\n",
       "                                          title-full  \n",
       "0  Monophyletic Relationship between Severe Acute...  \n",
       "1  Comprehensive overview of COVID-19 based on cu...  \n",
       "2  The SARS, MERS and novel coronavirus (COVID-19...  \n",
       "3  Evidence for zoonotic origins of Middle East r...  \n",
       "4             Deadly virus effortlessly hops species  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query string can be obtained from the `labelled_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 coronavirus origin\n"
     ]
    }
   ],
   "source": [
    "print(labelled_data[0][\"query_id\"], labelled_data[0][\"query\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compatible BERT encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are training a model that will be deployed in a search application, we need to ensure that the training encodings are compatible with encodings used at serving time. At serving time, document encodings will be applied offline when feeding the documents to the search engine while the query encoding will be applied at run-time upon arrival of the query. In addition, it might be relevant to use different maximum length for queries and documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert_encodings(queries, docs, tokenizer, query_input_size, doc_input_size):\n",
    "    queries_encodings = tokenizer(\n",
    "        queries, truncation=True, max_length=query_input_size-2, add_special_tokens=False\n",
    "    )\n",
    "    docs_encodings = tokenizer(\n",
    "        docs, truncation=True, max_length=doc_input_size-1, add_special_tokens=False\n",
    "    )\n",
    "    \n",
    "    TOKEN_NONE=0\n",
    "    TOKEN_CLS=101\n",
    "    TOKEN_SEP=102\n",
    "\n",
    "    input_ids = []\n",
    "    token_type_ids = []\n",
    "    attention_mask = []\n",
    "    for query_input_ids, doc_input_ids in zip(queries_encodings[\"input_ids\"], docs_encodings[\"input_ids\"]):\n",
    "        # create input id\n",
    "        input_id = [TOKEN_CLS] + query_input_ids + [TOKEN_SEP] + doc_input_ids + [TOKEN_SEP]\n",
    "        number_tokens = len(input_id)\n",
    "        padding_length = max(128 - number_tokens, 0)\n",
    "        input_id = input_id + [TOKEN_NONE] * padding_length\n",
    "        input_ids.append(input_id)\n",
    "        # create token id\n",
    "        token_type_id = [0] * len([TOKEN_CLS] + query_input_ids + [TOKEN_SEP]) + [1] * len(doc_input_ids + [TOKEN_SEP]) + [TOKEN_NONE] * padding_length\n",
    "        token_type_ids.append(token_type_id)\n",
    "        # create attention_mask\n",
    "        attention_mask.append([1] * number_tokens + [TOKEN_NONE] * padding_length)\n",
    "\n",
    "    encodings = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list for queries (represented by the query string), docs (represented by the doc titles) and labels from the `labelled_data` and `training_data` that we loaded earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_queries = []\n",
    "train_docs = []\n",
    "train_labels = []\n",
    "for data_point in labelled_data:\n",
    "    query_id = data_point[\"query_id\"]\n",
    "    titles = training_data[training_data[\"query_id\"] == query_id][\"title-full\"].tolist()\n",
    "    train_docs.extend(titles)\n",
    "    train_labels.extend([1 if x > 0 else 0 for x in training_data[training_data[\"query_id\"] == query_id][\"label\"].tolist()])\n",
    "    query = data_point[\"query\"]\n",
    "    train_queries.extend([query] * len(titles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a simple data split into train and validation sets for illustration purposes. The cord19 use case probably needs cross-validation to be used since it has only 50 queries containing relevance judgement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_queries, val_queries, train_docs, val_docs, train_labels, val_labels = train_test_split(\n",
    "    train_queries, train_docs, train_labels, test_size=.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train and validation encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/bert_uncased_L-4_H-512_A-8\"\n",
    "query_input_size=24\n",
    "doc_input_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "train_encodings = create_bert_encodings(\n",
    "    queries=train_queries, \n",
    "    docs=train_docs, \n",
    "    tokenizer=tokenizer, \n",
    "    query_input_size=query_input_size, \n",
    "    doc_input_size=doc_input_size\n",
    ")\n",
    "\n",
    "val_encodings = create_bert_encodings(\n",
    "    queries=val_queries, \n",
    "    docs=val_docs, \n",
    "    tokenizer=tokenizer, \n",
    "    query_input_size=query_input_size, \n",
    "    doc_input_size=doc_input_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Cord19Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = Cord19Dataset(train_encodings, train_labels)\n",
    "val_dataset = Cord19Dataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune the model (only task specific weights.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c850077b47eb437eac1d5c5781e8e2fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=116252865.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-4_H-512_A-8 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-512_A-8 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "babfbaa2950b48448bd21cf757a18932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdacbaae901a42ff812a191f6a4d0ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=7145.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8461690902709961, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0013995801259622112, 'total_flos': 3534603141120, 'step': 10}\n",
      "{'loss': 0.8681703567504883, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0027991602519244225, 'total_flos': 7069206282240, 'step': 20}\n",
      "{'loss': 0.857829475402832, 'learning_rate': 3e-06, 'epoch': 0.004198740377886634, 'total_flos': 10603809423360, 'step': 30}\n",
      "{'loss': 0.8333715438842774, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.005598320503848845, 'total_flos': 14138412564480, 'step': 40}\n",
      "{'loss': 0.8387279510498047, 'learning_rate': 5e-06, 'epoch': 0.006997900629811057, 'total_flos': 17673015705600, 'step': 50}\n",
      "{'loss': 0.8409191131591797, 'learning_rate': 6e-06, 'epoch': 0.008397480755773267, 'total_flos': 21207618846720, 'step': 60}\n",
      "{'loss': 0.8141700744628906, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.00979706088173548, 'total_flos': 24742221987840, 'step': 70}\n",
      "{'loss': 0.7777870178222657, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01119664100769769, 'total_flos': 28276825128960, 'step': 80}\n",
      "{'loss': 0.759039306640625, 'learning_rate': 9e-06, 'epoch': 0.012596221133659902, 'total_flos': 31811428270080, 'step': 90}\n",
      "{'loss': 0.7358154296875, 'learning_rate': 1e-05, 'epoch': 0.013995801259622114, 'total_flos': 35346031411200, 'step': 100}\n",
      "{'loss': 0.7224632263183594, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.015395381385584325, 'total_flos': 38880634552320, 'step': 110}\n",
      "{'loss': 0.7000160217285156, 'learning_rate': 1.2e-05, 'epoch': 0.016794961511546535, 'total_flos': 42415237693440, 'step': 120}\n",
      "{'loss': 0.6806869506835938, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.01819454163750875, 'total_flos': 45949840834560, 'step': 130}\n",
      "{'loss': 0.6666877746582032, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.01959412176347096, 'total_flos': 49484443975680, 'step': 140}\n",
      "{'loss': 0.6240447998046875, 'learning_rate': 1.5e-05, 'epoch': 0.02099370188943317, 'total_flos': 53019047116800, 'step': 150}\n",
      "{'loss': 0.5942115783691406, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.02239328201539538, 'total_flos': 56553650257920, 'step': 160}\n",
      "{'loss': 0.5723480224609375, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.023792862141357594, 'total_flos': 60088253399040, 'step': 170}\n",
      "{'loss': 0.5292701721191406, 'learning_rate': 1.8e-05, 'epoch': 0.025192442267319804, 'total_flos': 63622856540160, 'step': 180}\n",
      "{'loss': 0.5330642700195313, 'learning_rate': 1.9e-05, 'epoch': 0.026592022393282014, 'total_flos': 67157459681280, 'step': 190}\n",
      "{'loss': 0.46725311279296877, 'learning_rate': 2e-05, 'epoch': 0.02799160251924423, 'total_flos': 70692062822400, 'step': 200}\n",
      "{'loss': 0.442303466796875, 'learning_rate': 2.1e-05, 'epoch': 0.02939118264520644, 'total_flos': 74226665963520, 'step': 210}\n",
      "{'loss': 0.4489776611328125, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.03079076277116865, 'total_flos': 77761269104640, 'step': 220}\n",
      "{'loss': 0.411376953125, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.03219034289713086, 'total_flos': 81295872245760, 'step': 230}\n",
      "{'loss': 0.35503387451171875, 'learning_rate': 2.4e-05, 'epoch': 0.03358992302309307, 'total_flos': 84830475386880, 'step': 240}\n",
      "{'loss': 0.40528106689453125, 'learning_rate': 2.5e-05, 'epoch': 0.03498950314905528, 'total_flos': 88365078528000, 'step': 250}\n",
      "{'loss': 0.32529296875, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.0363890832750175, 'total_flos': 91899681669120, 'step': 260}\n",
      "{'loss': 0.3455596923828125, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.03778866340097971, 'total_flos': 95434284810240, 'step': 270}\n",
      "{'loss': 0.3091766357421875, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.03918824352694192, 'total_flos': 98968887951360, 'step': 280}\n",
      "{'loss': 0.27514801025390623, 'learning_rate': 2.9e-05, 'epoch': 0.04058782365290413, 'total_flos': 102503491092480, 'step': 290}\n",
      "{'loss': 0.37311553955078125, 'learning_rate': 3e-05, 'epoch': 0.04198740377886634, 'total_flos': 106038094233600, 'step': 300}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the model to onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.onnx import export\n",
    "from pathlib import Path \n",
    "\n",
    "model_onnx_path = Path(model_name + \".onnx\")\n",
    "dummy_input = (\n",
    "    train_dataset[0][\"input_ids\"].unsqueeze(0), \n",
    "    train_dataset[0][\"token_type_ids\"].unsqueeze(0), \n",
    "    train_dataset[0][\"attention_mask\"].unsqueeze(0)\n",
    ")\n",
    "input_names = [\"input_ids\", \"token_type_ids\", \"attention_mask\"]\n",
    "output_names = [\"logits\"]\n",
    "export(\n",
    "    model, dummy_input, model_onnx_path, input_names = input_names, \n",
    "    output_names = output_names, verbose=False, opset_version=11\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check output type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "m = ort.InferenceSession(model_name + \".onnx\") \n",
    "print(m.get_outputs()[0].name)\n",
    "print(m.get_outputs()[0].type)\n",
    "print(m.get_outputs()[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another form to check output type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx                                                                                                                                                                          \n",
    "m = onnx.load(model_name + \".onnx\")                                                                                                                                                         \n",
    "m.graph.output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
